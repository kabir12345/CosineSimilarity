{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPLAB9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lcYwDXtDmwR"
      },
      "source": [
        "Mining artciles from the internet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEMjkpDt8fAe",
        "outputId": "627b1b84-9bb0-446a-e288-a74bba14de92"
      },
      "source": [
        "!pip install newspaper3k\n",
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "pagesToGet= 1\n",
        "\n",
        "upperframe=[]  \n",
        "for page in range(1,pagesToGet+1):\n",
        "    print('processing page :', page)\n",
        "    url = 'https://www.politifact.com/factchecks/list/?page='+str(page)\n",
        "    print(url)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "       \n",
        "        page=requests.get(url)                            \n",
        "    \n",
        "    except Exception as e:                                  \n",
        "        error_type, error_obj, error_info = sys.exc_info()       \n",
        "        print ('ERROR FOR LINK:',url)                           \n",
        "        print (error_type, 'Line:', error_info.tb_lineno)      \n",
        "        continue                                              \n",
        "    time.sleep(2)   \n",
        "    soup=BeautifulSoup(page.text,'html.parser')\n",
        "    frame=[]\n",
        "    links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
        "    print(len(links))\n",
        "    filename=\"NEWS.csv\"\n",
        "    f=open(filename,\"w\", encoding = 'utf-8')\n",
        "    headers=\"Statement,Link,Date, Source, Label\\n\"\n",
        "    f.write(headers)\n",
        "    \n",
        "    for j in links:\n",
        "        Statement = j.find(\"div\",attrs={'class':'m-statement__quote'}).text.strip()\n",
        "        Link = \"https://www.politifact.com\"\n",
        "        Link += j.find(\"div\",attrs={'class':'m-statement__quote'}).find('a')['href'].strip()\n",
        "        Date = j.find('div',attrs={'class':'m-statement__body'}).find('footer').text[-14:-1].strip()\n",
        "        Source = j.find('div', attrs={'class':'m-statement__meta'}).find('a').text.strip()\n",
        "        Label = j.find('div', attrs ={'class':'m-statement__content'}).find('img',attrs={'class':'c-image__original'}).get('alt').strip()\n",
        "        frame.append((Statement,Link,Date,Source,Label))\n",
        "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\",\"+Source.replace(\",\",\"^\")+\",\"+Label.replace(\",\",\"^\")+\"\\n\")\n",
        "    upperframe.extend(frame)\n",
        "f.close()\n",
        "data=pd.DataFrame(upperframe, columns=['Statement','Link','Date','Source','Label'])\n",
        "dfl=data.head()\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 20.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 12.2MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.6MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.7MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=ca65302867b894a2560539aff3a82921a7dd880f98dbe70416ff7857e57d2e2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=930c3dfdd80c521f02abe9bf1e92b841a900834d1bc16a877b8560ba674cd68e\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=06d0eeffe84aaad2b221f774ad21bb9fa0b3e4dd9ae163b59eee193b754a7697\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=13cec7d609325e97e9b61471715f3fb658e96743ef9b83e0f311e036f3fc9c83\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, tinysegmenter, requests-file, tldextract, cssselect, feedfinder2, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n",
            "processing page : 1\n",
            "https://www.politifact.com/factchecks/list/?page=1\n",
            "30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Statement', 'Link', 'Date', 'Source', 'Label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB9e-caY8ENy",
        "outputId": "3d26b458-e0cd-4a03-e5a8-72013c311b00"
      },
      "source": [
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "articles=[]\n",
        "for i in range(5):\n",
        "  articles.append(data.Link[i])\n",
        "print(articles)\n",
        "a_text=[]\n",
        "Titles = []\n",
        "for l in articles:\n",
        "  article = Article(l, language=\"en\")\n",
        "  article.download()                      \n",
        "  article.parse()                         \n",
        "  article.nlp()                           \n",
        "  a_text.append(article.text)           \n",
        "  Titles.append(article.title)  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['https://www.politifact.com/factchecks/2020/nov/23/tim-murtaugh/tim-murtaugh-wrongly-says-key-pennsylvania-mail-ba/', 'https://www.politifact.com/factchecks/2020/nov/21/joe-sanfelippo/voters-not-clerks-decided-who-was-indefinitely-con/', 'https://www.politifact.com/factchecks/2020/nov/20/sidney-powell/sidney-powell-claim-450000-votes-were-only-biden-k/', 'https://www.politifact.com/factchecks/2020/nov/20/rudy-giuliani/giuliani-cites-affidavit-crucial-errors-press-conf/', 'https://www.politifact.com/factchecks/2020/nov/20/joe-biden/biden-mischaracterizes-teacher-layoffs-pandemic/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Y0mC1GDupE"
      },
      "source": [
        "Preprocessing of the article data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6OYItY8Udg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86b9e07-097c-4230-ebfd-32227825c99a"
      },
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "def convert_lower_case(data):\n",
        "    return np.char.lower(data)\n",
        "def remove_stop_words(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words and len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return new_text\n",
        "def remove_punctuation(data):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        data = np.char.replace(data, symbols[i], ' ')\n",
        "        data = np.char.replace(data, \"  \", \" \")\n",
        "    data = np.char.replace(data, ',', '')\n",
        "    data = np.char.replace(data, \"'\", \"\")\n",
        "    return data\n",
        "def lemmatization(data):\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    \n",
        "    tokens = word_tokenize(str(data))\n",
        "    new_text = \"\"\n",
        "    for w in tokens:\n",
        "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
        "    return new_text\n",
        "def preprocess(data):\n",
        "    data = convert_lower_case(data)     \n",
        "    data = remove_punctuation(data)       \n",
        "    data = remove_stop_words(data)        \n",
        "    data = lemmatization(data)            \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLx6fqIbCIpK"
      },
      "source": [
        "n_atext = []\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  n_article = preprocess(a_text[i])\n",
        "\n",
        "  n_atext.append(n_article)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIPR--NKCSOe"
      },
      "source": [
        "article_ids = [\"Article_\" + str(i) for i in range(5)]\n",
        "\n",
        "article_dict = dict(zip(article_ids, Titles))\n",
        "\n",
        "ids = list(article_dict.keys())\n",
        "\n",
        "pairs = []\n",
        "for i, v in enumerate(ids):\n",
        "    for j in ids[i+1:]:\n",
        "        pairs.append((ids[i], j))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEFguPNkD2Xk"
      },
      "source": [
        "Calculating Cosine similarity between articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "oJK_egZSDJMX",
        "outputId": "452a940e-142d-47c9-9c1a-bcbb060baa33"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,1))\n",
        "vectorizer\n",
        "feature_matrix = vectorizer.fit_transform(n_atext).astype(float)\n",
        "def compute_cosine_similarity(pair):\n",
        "    \n",
        "    article1, article2 = pair\n",
        "    \n",
        "    article1_index = int(article1.split(\"_\")[1])\n",
        "    article2_index = int(article2.split(\"_\")[1])\n",
        "    \n",
        "    article1_fm = feature_matrix.toarray()[article1_index]\n",
        "    article2_fm = feature_matrix.toarray()[article2_index]\n",
        "    \n",
        "    manual_cosine_similarity = np.dot(article1_fm, article2_fm)\n",
        "    \n",
        "    return manual_cosine_similarity\n",
        "cosine_similarity = [compute_cosine_similarity(pair) for pair in pairs]\n",
        "df = pd.DataFrame({'pair': pairs, 'similarity': cosine_similarity})\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pair</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(Article_0, Article_1)</td>\n",
              "      <td>0.112471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Article_0, Article_2)</td>\n",
              "      <td>0.177160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(Article_0, Article_3)</td>\n",
              "      <td>0.111870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(Article_0, Article_4)</td>\n",
              "      <td>0.029453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Article_1, Article_2)</td>\n",
              "      <td>0.163870</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     pair  similarity\n",
              "0  (Article_0, Article_1)    0.112471\n",
              "1  (Article_0, Article_2)    0.177160\n",
              "2  (Article_0, Article_3)    0.111870\n",
              "3  (Article_0, Article_4)    0.029453\n",
              "4  (Article_1, Article_2)    0.163870"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}